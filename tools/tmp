
import torch.nn.functional as F
class CWDLoss(nn.Module):
    # tau用来控制熵的大小，tau越大
    # 当 tau 较大时，Softmax 的输出分布会更平滑（熵更高），
    # 不同类别之间的概率差异会缩小，相当于“软化”了目标分布。
    # 当 tau 较小时，Softmax 的输出分布会更尖锐（熵更低），
    # 概率高的类别会更突出，相当于“硬化”了目标分布

    # 在 CWD Loss 的公式中，tau 不仅仅是一个简单的缩放因子，
    # 它在 Softmax 函数内部起作用，改变了概率分布的形态。
    # 直接对最终的 cost 进行缩放，虽然也能改变损失值的大小，
    # 但并不能达到调整学生模型学习教师模型概率分布“软硬”程度的目的
    def __init__(self, channel_t, channel_s, tau=1.0):
        super().__init__()
        self.tau = tau

    # 多个feature对齐
    def forward(self, feats_t : list, feats_s : list):
        assert(len(feats_t) == len(feats_s))
        
        loss = []
        # feat shape: b,c,h,w
        for feat_t, feat_s in zip(feats_t, feats_s):
            assert(feat_t.shape == feat_s.shape)
            
            b, c, h, w = feat_t.shape

            # view将feat resize成两个维度，指定了后一个维度由wxh构成
            # sotmax只能在tensor的一个维度计算，因此将feat用view展开成-1（自动计算），w*h两个维度 来计算w*h
            # 补充：因为CWD思想是通道间存在不同信息，因此没有将feat直接展开成一维而是b*c作为通道维度，w*h作为数据维度
            softmax_t = F.softmax(feat_t.view(-1, h*w) / self.tau, dim=1)
            print("softmax:", softmax_t)
            
            # 和上面一样，指定了第二个维度；LogSoftmax和F.softmax不同，通常在forward充当nn.Module对象
            softmax_layer = torch.nn.LogSoftmax(dim=1)
            # 对数运算法则，log(a/b) = loga - logb
            cost = torch.sum(
                softmax_t * 
                (
                    softmax_layer(feat_t.view(-1, h*w) / self.tau) -
                    softmax_layer(feat_s.view(-1, h*w) / self.tau)
                )) * self.tau ** 2

            # 将相对熵除bc得到每个batch的每个channel下（即维度级别）的损失值
            loss.append(cost / (b*c))

        loss = sum(loss)
        return loss

# 蒸馏器的预处理，包括
class loss_distillation_feature(nn.Module):
    def __init__(self, feats_t, feats_s, distiller='CWD', loss_weight=1.0):
        super().__init__()
        self.distiller = distiller
        self.loss_weight = loss_weight

        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.align_module = nn.ModuleList([
            nn.Conv2d(feat_s, feat_t, kernel_size=1, stride=1, padding=0).to(device) 
            for feat_t, feat_s in zip(feats_t, feats_s)
        ])

        self.norm = [
            nn.BatchNorm2d(feat_t, affine=False).to(device)
            for feat_t in feats_t
        ]
        
        print('channels_s_num : ', feats_s)
        print('channels_T_num : ', feats_t)

        if distiller == 'CWD':
            self.feature_loss = CWDLoss(feats_t, feats_s)
        else:
            raise NotImplementedError
        
    def forward(self, feats_t, feats_s):
        assert len(feats_t) == len(feats_s)
        cnnt_t = []
        cnnt_s = []

        for id, (feat_t, feat_s) in enumerate(zip(feats_t, feats_s)):
            if self.distiller == "CWD":
                # 通过每个通道的一个Conv2d连接学生和教师
                feat_s = self.align_module[id](feat_s)
                feat_s = self.norm[id](feat_s)
                feat_t = self.norm[id](feat_t)

            cnnt_t.append(feat_t)
            cnnt_s.append(feat_s)

        loss = self.feature_loss(cnnt_t, cnnt_s)
        return self.loss_weight * loss


from ultralytics.nn.modules import C2f
class loss_disillation:
    def __init__(self, model_t, model_s, loss_type='CWDLoss'):
        self.distiller = loss_type[:3]

        # use C2f.cv2 conly
        # both teacher and student are yolov8 with same structure
        self.match_layer_id = [15, 18, 21]

        # filter output feat
        channels_t = []
        channels_s = []

        seq = model_t.model.model
        for i in self.match_layer_id:
            if isinstance(seq[i], C2f):
                channels_t.append(seq[i].cv2.conv.out_channels)

        seq = model_s.model
        for i in self.match_layer_id:
            if isinstance(seq[i], C2f):
                channels_s.append(seq[i].cv2.conv.out_channels)

        self.loss_distillation_feature_ = loss_distillation_feature(channels_t, channels_s, distiller=self.distiller)

        self.paired_t = []
        self.paired_s = []
        self.remove_handle = []


        seq = model_t.model.model
        for i in self.match_layer_id:
            if isinstance(seq[i], C2f):
                self.paired_t.append(seq[i])

        seq = model_s.model
        for i in self.match_layer_id:
            if isinstance(seq[i], C2f):
                self.paired_s.append(seq[i])


    def register_hook(self):
        self.outputs_t = []
        self.outputs_s = []

        def make_student_layer_forward_hook(output_list):
            def forward_hook(m, input, output):
                output_list.append(output)

            return forward_hook
        
        def make_teacher_layer_forward_hook(output_list):
            def forward_hook(m, input, output):
                output_list.append(output)

            return forward_hook

        for pair_t, pair_s in zip(self.paired_t, self.paired_s):
            self.remove_handle.append(pair_t.register_forward_hook(make_teacher_layer_forward_hook(self.outputs_t)))
            self.remove_handle.append(pair_s.register_forward_hook(make_student_layer_forward_hook(self.outputs_s)))
        
    # TODO
    def get_loss(self):
        quant_loss = 0
        # for index, (mo, fo) in enumerate(zip(self.teacher_outputs, self.origin_outputs)):
        #     print(mo.shape,fo.shape)
        # quant_loss += self.loss_distillation_feature_(mo, fo)

        if not self.outputs_t or not self.outputs_s:
            print(
                f"Warning: output not defined outputs - Teacher: {len(self.outputs_t)}, Student: {len(self.outputs_s)}")
            self.outputs_t.clear() 
            self.outputs_s.clear()
            return torch.tensor(0.0, requires_grad=True), False

        if len(self.outputs_t) != len(self.outputs_s):
            print(
                f"Warning: Mismatched outputs - Teacher: {len(self.outputs_t)}, Student: {len(self.outputs_s)}")
            self.outputs_t.clear() 
            self.outputs_s.clear()
            return torch.tensor(0.0, requires_grad=True), False
        
        if len(self.outputs_t) > len(self.match_layer_id):
            self.teacher_outputs = self.teacher_outputs[len(self.match_layer_id):]

        quant_loss += self.loss_distillation_feature_(feats_t=self.outputs_t, feats_s=self.outputs_s)
        if self.distiller != 'CWD':
            quant_loss *= 0.3
        self.outputs_t.clear()
        self.outputs_s.clear()
        return quant_loss, True

    def remove_handle_(self):
        for rm in self.remove_handle:
            rm.remove()

class BaseTrainer:
    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        # 稀疏训练 
        if overrides and "prune" in overrides:
            self.prune = overrides["prune"]
            overrides.pop("prune")
        else:
            self.prune = False
        
        # 回调训练
        if overrides and "prune_finetune" in overrides:
            self.prune_finetune = overrides["prune_finetune"]
            overrides.pop("prune_finetune")
        else:
            self.prune_finetune = False

        # 教师模型
        if overrides and "teacher" in overrides:
            self.teacher = overrides["teacher"]
            overrides.pop("teacher")
        else:
            self.teacher = None

        # 蒸馏损失函数
        if overrides and "distillation_loss" in overrides:
            self.distillation_loss = overrides["distillation_loss"]
            overrides.pop("distillation_loss")
        else:
            self.distillation_loss = None

        self.args = get_cfg(cfg, overrides)
        self.check_resume(overrides)
        self.device = select_device(self.args.device, self.args.batch)
        self.validator = None
        self.metrics = None
        self.plots = {}
        init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)

        # Dirs
        self.save_dir = get_save_dir(self.args)
        self.args.name = self.save_dir.name  # update name for loggers
        self.wdir = self.save_dir / "weights"  # weights dir
        if RANK in {-1, 0}:
            self.wdir.mkdir(parents=True, exist_ok=True)  # make dir
            self.args.save_dir = str(self.save_dir)
            yaml_save(self.save_dir / "args.yaml", vars(self.args))  # save run args
        self.last, self.best = self.wdir / "last.pt", self.wdir / "best.pt"  # checkpoint paths
        self.save_period = self.args.save_period

        self.batch_size = self.args.batch
        self.epochs = self.args.epochs or 100  # in case users accidentally pass epochs=None with timed training
        self.start_epoch = 0
        if RANK == -1:
            print_args(vars(self.args))

        # Device
        if self.device.type in {"cpu", "mps"}:
            self.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading

        # Model and Dataset
        self.model = check_model_file_from_stem(self.args.model)  # add suffix, i.e. yolo11n -> yolo11n.pt
        with torch_distributed_zero_first(LOCAL_RANK):  # avoid auto-downloading dataset multiple times
            self.trainset, self.testset = self.get_dataset()
        self.ema = None

        # Optimization utils init
        self.lf = None
        self.scheduler = None

        # Epoch level metrics
        self.best_fitness = None
        self.fitness = None
        self.loss = None
        self.tloss = None
        self.loss_names = ["Loss"]
        self.csv = self.save_dir / "results.csv"
        self.plot_idx = [0, 1, 2]

        # HUB
        self.hub_session = None

        # Callbacks
        self.callbacks = _callbacks or callbacks.get_default_callbacks()
        if RANK in {-1, 0}:
            callbacks.add_integration_callbacks(self)

    def train(self):
        """Allow device='', device=None on Multi-GPU systems to default to device=0."""
        if isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'
            world_size = len(self.args.device.split(","))
        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)
            world_size = len(self.args.device)
        elif self.args.device in {"cpu", "mps"}:  # i.e. device='cpu' or 'mps'
            world_size = 0
        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=number
            world_size = 1  # default to device 0
        else:  # i.e. device=None or device=''
            world_size = 0

        # Run subprocess if DDP training, else train normally
        if world_size > 1 and "LOCAL_RANK" not in os.environ:
            # Argument checks
            if self.args.rect:
                LOGGER.warning("'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'")
                self.args.rect = False
            if self.args.batch < 1.0:
                LOGGER.warning(
                    "'batch<1' for AutoBatch is incompatible with Multi-GPU training, setting default 'batch=16'"
                )
                self.args.batch = 16

            # Command
            cmd, file = generate_ddp_command(world_size, self)
            try:
                LOGGER.info(f"{colorstr('DDP:')} debug command {' '.join(cmd)}")
                subprocess.run(cmd, check=True)
            except Exception as e:
                raise e
            finally:
                ddp_cleanup(self, str(file))

        else:
            self._do_train(world_size)

    def _setup_scheduler(self):
        """Initialize training learning rate scheduler."""
        if self.args.cos_lr:
            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1->hyp['lrf']
        else:
            self.lf = lambda x: max(1 - x / self.epochs, 0) * (1.0 - self.args.lrf) + self.args.lrf  # linear
        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)

    def _setup_ddp(self, world_size):
        """Initialize and set the DistributedDataParallel parameters for training."""
        torch.cuda.set_device(RANK)
        self.device = torch.device("cuda", RANK)
        # LOGGER.info(f'DDP info: RANK {RANK}, WORLD_SIZE {world_size}, DEVICE {self.device}')
        os.environ["TORCH_NCCL_BLOCKING_WAIT"] = "1"  # set to enforce timeout
        dist.init_process_group(
            backend="nccl" if dist.is_nccl_available() else "gloo",
            timeout=timedelta(seconds=10800),  # 3 hours
            rank=RANK,
            world_size=world_size,
        )

    def _setup_train(self, world_size):
        """Build dataloaders and optimizer on correct rank process."""
        # Model
        self.run_callbacks("on_pretrain_routine_start")
        ckpt = self.setup_model()
        self.model = self.model.to(self.device)
        self.set_model_attributes()

        # Freeze layers
        freeze_list = (
            self.args.freeze
            if isinstance(self.args.freeze, list)
            else range(self.args.freeze)
            if isinstance(self.args.freeze, int)
            else []
        )
        always_freeze_names = [".dfl"]  # always freeze these layers
        freeze_layer_names = [f"model.{x}." for x in freeze_list] + always_freeze_names
        self.freeze_layer_names = freeze_layer_names
        for k, v in self.model.named_parameters():
            # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)
            if any(x in k for x in freeze_layer_names):
                LOGGER.info(f"Freezing layer '{k}'")
                v.requires_grad = False
            elif not v.requires_grad and v.dtype.is_floating_point:  # only floating point Tensor can require gradients
                LOGGER.warning(
                    f"setting 'requires_grad=True' for frozen layer '{k}'. "
                    "See ultralytics.engine.trainer for customization of frozen layers."
                )
                v.requires_grad = True

        # Check AMP
        self.amp = torch.tensor(self.args.amp).to(self.device)  # True or False
        if self.amp and RANK in {-1, 0}:  # Single-GPU and DDP
            callbacks_backup = callbacks.default_callbacks.copy()  # backup callbacks as check_amp() resets them
            self.amp = torch.tensor(check_amp(self.model), device=self.device)
            callbacks.default_callbacks = callbacks_backup  # restore callbacks
        if RANK > -1 and world_size > 1:  # DDP
            dist.broadcast(self.amp.int(), src=0)  # broadcast from rank 0 to all other ranks; gloo errors with boolean
        self.amp = bool(self.amp)  # as boolean
        self.scaler = (
            torch.amp.GradScaler("cuda", enabled=self.amp) if TORCH_2_4 else torch.cuda.amp.GradScaler(enabled=self.amp)
        )
        if world_size > 1:
            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[RANK], find_unused_parameters=True)

            # 教师模型移动至device
            if not self.teacher is None:
                self.teacher = nn.parallel.DistributedDataParallel(self.teacher, device_ids=[RANK])
                self.teacher.eval()

        # Check imgsz
        gs = max(int(self.model.stride.max() if hasattr(self.model, "stride") else 32), 32)  # grid size (max stride)
        self.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)
        self.stride = gs  # for multiscale training

        # Batch size
        if self.batch_size < 1 and RANK == -1:  # single-GPU only, estimate best batch size
            self.args.batch = self.batch_size = self.auto_batch()

        # Dataloaders
        batch_size = self.batch_size // max(world_size, 1)
        self.train_loader = self.get_dataloader(self.trainset, batch_size=batch_size, rank=LOCAL_RANK, mode="train")
        if RANK in {-1, 0}:
            # Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.
            self.test_loader = self.get_dataloader(
                self.testset, batch_size=batch_size if self.args.task == "obb" else batch_size * 2, rank=-1, mode="val"
            )
            self.validator = self.get_validator()
            metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix="val")
            self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))
            self.ema = ModelEMA(self.model)
            if self.args.plots:
                self.plot_training_labels()

        # Optimizer
        self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing
        weight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay
        iterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs
        self.optimizer = self.build_optimizer(
            model=self.model,
            name=self.args.optimizer,
            lr=self.args.lr0,
            momentum=self.args.momentum,
            decay=weight_decay,
            iterations=iterations,
        )
        # Scheduler
        self._setup_scheduler()
        self.stopper, self.stop = EarlyStopping(patience=self.args.patience), False
        self.resume_training(ckpt)
        self.scheduler.last_epoch = self.start_epoch - 1  # do not move
        self.run_callbacks("on_pretrain_routine_end")

    def _do_train(self, world_size=1):
        """Train the model with the specified world size."""
        if world_size > 1:
            self._setup_ddp(world_size)
        self._setup_train(world_size)

        nb = len(self.train_loader)  # number of batches
        nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1  # warmup iterations
        last_opt_step = -1
        self.epoch_time = None
        self.epoch_time_start = time.time()
        self.train_time_start = time.time()
        self.run_callbacks("on_train_start")
        LOGGER.info(
            f"Image sizes {self.args.imgsz} train, {self.args.imgsz} val\n"
            f"Using {self.train_loader.num_workers * (world_size or 1)} dataloader workers\n"
            f"Logging results to {colorstr('bold', self.save_dir)}\n"
            f"Starting training for " + (f"{self.args.time} hours..." if self.args.time else f"{self.epochs} epochs...")
        )
        if self.args.close_mosaic:
            base_idx = (self.epochs - self.args.close_mosaic) * nb
            self.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])

        # pipeline增加蒸馏损失
        if not self.teacher is None:
            loss_disillation_ = loss_disillation(self.teacher, self.model, loss_type=self.distillation_loss)

        epoch = self.start_epoch
        self.optimizer.zero_grad()  # zero any resumed gradients to ensure stability on train start
        while True:
			# 模型训练前注册hook
            if not self.teacher is None:
                loss_disillation_.register_hook()

            self.epoch = epoch
            self.run_callbacks("on_train_epoch_start")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")  # suppress 'Detected lr_scheduler.step() before optimizer.step()'
                self.scheduler.step()

            self._model_train()
            if RANK != -1:
                self.train_loader.sampler.set_epoch(epoch)
            pbar = enumerate(self.train_loader)
            # Update dataloader attributes (optional)
            if epoch == (self.epochs - self.args.close_mosaic):
                self._close_dataloader_mosaic()
                self.train_loader.reset()

            if RANK in {-1, 0}:
                LOGGER.info(self.progress_string())
                pbar = TQDM(enumerate(self.train_loader), total=nb)
            self.tloss = None
            for i, batch in pbar:
                self.run_callbacks("on_train_batch_start")
                # Warmup
                ni = i + nb * epoch
                if ni <= nw:
                    xi = [0, nw]  # x interp
                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round()))
                    for j, x in enumerate(self.optimizer.param_groups):
                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                        x["lr"] = np.interp(
                            ni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x["initial_lr"] * self.lf(epoch)]
                        )
                        if "momentum" in x:
                            x["momentum"] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])

                # Forward
                with autocast(self.amp):
                    batch = self.preprocess_batch(batch)
                    loss, self.loss_items = self.model(batch)
                    self.loss = loss.sum()
                    if RANK != -1:
                        self.loss *= world_size
                    self.tloss = (
                        (self.tloss * i + self.loss_items) / (i + 1) if self.tloss is not None else self.loss_items
                    )

                    if not self.teacher is None:
                        distillation_weight = ((1 - math.cos(i * math.pi / len(self.train_loader))) / 2) * (0.1 - 1) + 1
                        with torch.no_grad():
                            # 预测关闭打印
                            pred = self.teacher(batch['img'], verbose=False)
                        
                        self.loss_d, flg = loss_disillation_.get_loss()

                        if (flg):
                            self.loss_d *= distillation_weight
                            if i == 0:
                                print(self.loss_d, '-------------------------')
                                print(self.loss, '-------------------------')

                            self.loss += self.loss_d
                # Backward
                self.scaler.scale(self.loss).backward()

                if (self.prune):
                    # 计算l1正则化系数强度，设置1e-2这样较小的初始化强度
                    # (1 - 0.9 * epoch / self.epochs) 随着训练次数逐渐减小（由1-0.1），l1越来越小（0.1-0.01），这种策略被称为稀疏性退火
                    l1_lambda = 1e-3 * (1 - 0.9 * epoch / self.epochs)
                    ## 遍历所有模块找是不是BN模块
                    for k, m in self.model.named_modules():
                        if isinstance(m, nn.BatchNorm2d):
                            # 对BN层权重引用l1
                            # torch.sign(m.weight.data) 返回一个和 m.weight.data 相同形状的张量，其元素值为1、-1、0 ，起到mask作用
                            m.weight.grad.data.add_(l1_lambda * torch.sign(m.weight.data))
                            # 这里使用了固定的 L1 正则化强度 1e-2
                            m.bias.grad.data.add_(1e-2 * torch.sign(m.bias.data))

                # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html
                if ni - last_opt_step >= self.accumulate:
                    self.optimizer_step()
                    last_opt_step = ni

                    # Timed stopping
                    if self.args.time:
                        self.stop = (time.time() - self.train_time_start) > (self.args.time * 3600)
                        if RANK != -1:  # if DDP training
                            broadcast_list = [self.stop if RANK == 0 else None]
                            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
                            self.stop = broadcast_list[0]
                        if self.stop:  # training time exceeded
                            break
                # Log
                if RANK in {-1, 0}:
                    loss_length = self.tloss.shape[0] if len(self.tloss.shape) else 1
                    pbar.set_description(
                        ("%11s" * 2 + "%11.4g" * (2 + loss_length))
                        % (
                            f"{epoch + 1}/{self.epochs}",
                            f"{self._get_memory():.3g}G",  # (GB) GPU memory util
                            *(self.tloss if loss_length > 1 else torch.unsqueeze(self.tloss, 0)),  # losses
                            batch["cls"].shape[0],  # batch size, i.e. 8
                            batch["img"].shape[-1],  # imgsz, i.e 640
                        )
                    )
                    self.run_callbacks("on_batch_end")
                    if self.args.plots and ni in self.plot_idx:
                        self.plot_training_samples(batch, ni)

                self.run_callbacks("on_train_batch_end")

                # 去掉hook
                if not self.teacher is None:
                    loss_disillation_.remove_handle_()

            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers
            self.run_callbacks("on_train_epoch_end")
            if RANK in {-1, 0}:
                final_epoch = epoch + 1 >= self.epochs
                self.ema.update_attr(self.model, include=["yaml", "nc", "args", "names", "stride", "class_weights"])

                # Validation
                if self.args.val or final_epoch or self.stopper.possible_stop or self.stop:
                    self.metrics, self.fitness = self.validate()
                self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})
                self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch
                if self.args.time:
                    self.stop |= (time.time() - self.train_time_start) > (self.args.time * 3600)

                # Save model
                if self.args.save or final_epoch:
                    self.save_model()
                    self.run_callbacks("on_model_save")

            # Scheduler
            t = time.time()
            self.epoch_time = t - self.epoch_time_start
            self.epoch_time_start = t
            if self.args.time:
                mean_epoch_time = (t - self.train_time_start) / (epoch - self.start_epoch + 1)
                self.epochs = self.args.epochs = math.ceil(self.args.time * 3600 / mean_epoch_time)
                self._setup_scheduler()
                self.scheduler.last_epoch = self.epoch  # do not move
                self.stop |= epoch >= self.epochs  # stop if exceeded epochs
            self.run_callbacks("on_fit_epoch_end")
            if self._get_memory(fraction=True) > 0.5:
                self._clear_memory()  # clear if memory utilization > 50%

            # Early Stopping
            if RANK != -1:  # if DDP training
                broadcast_list = [self.stop if RANK == 0 else None]
                dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
                self.stop = broadcast_list[0]
            if self.stop:
                break  # must break all DDP ranks
            epoch += 1

        if RANK in {-1, 0}:
            # Do final val with best.pt
            seconds = time.time() - self.train_time_start
            LOGGER.info(f"\n{epoch - self.start_epoch + 1} epochs completed in {seconds / 3600:.3f} hours.")
            self.final_eval()
            if self.args.plots:
                self.plot_metrics()
            self.run_callbacks("on_train_end")
        self._clear_memory()
        unset_deterministic()
        self.run_callbacks("teardown")